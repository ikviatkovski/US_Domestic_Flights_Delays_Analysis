{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERAL ASSEMBLY NYC #\n",
    "***\n",
    "## Data Science Immersive Course ##\n",
    "***\n",
    "## Capstone Project ##\n",
    "***\n",
    "# Project's Name #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Files ##\n",
    "\n",
    "The project consists of a number of separate Jupyter Notebooks listed in the order of their workflow sequence and available at the __[code](http://localhost:8888/tree/PROJECTS/cstone/code)__ folder in the Project repository:\n",
    "\n",
    "1. __[1 Importing Data](http://localhost:8888/notebooks/PROJECTS/cstone/code/1%20Importing%20data.ipynb)__ - a notebook with initial data import from internet databases\n",
    "2. __[2 Cleaning](http://localhost:8888/notebooks/PROJECTS/cstone/code/2%20Cleaning.ipynb)__ - a notebook with the code for data cleaning and some insight on the initial data structure\n",
    "3. __[3 EDA](http://localhost:8888/notebooks/PROJECTS/cstone/code/3%20EDA.ipynb)__ - a notebook with the extensive Exploratory Data Analysis\n",
    "4. __[4 Pre-Processing](http://localhost:8888/notebooks/PROJECTS/cstone/code/4%20Pre-Processing.ipynb#Baseline-Model)__ - a notebook with data preparation for modeling and setting the Baseline model for classification\n",
    "5. __[5 Modeling Logistic Regression](http://localhost:8888/notebooks/PROJECTS/cstone/code/5%20Modeling%20Logistic%20Regression.ipynb#Importing-Data-and-Initial-Checks)__ - a notebook with Logistic Regression Classification model\n",
    "6. __[6 Modeling Decision Trees](http://localhost:8888/notebooks/PROJECTS/cstone/code/6%20Modeling%20Decision%20Trees.ipynb)__ - a notebook with Decision Trees family models\n",
    "7. __[7 Modeling FFNN Classifier](http://localhost:8888/notebooks/PROJECTS/cstone/code/7%20Modeling%20FFNN%20%20Classifier.ipynb)__ - a notebook with Forwared Feeding Neural Network Classification model\n",
    "8. __[8 Modeling FFNN Regressor](http://localhost:8888/notebooks/PROJECTS/cstone/code/8%20Modeling%20FFNN%20Regressor.ipynb)__ - a notebook with Forwared Feeding Neural Network Regression model\n",
    "\n",
    "Datasets and auxillary data files used for the Project and generated through the Project exceed any available external storage and are stored locally only.\n",
    "\n",
    "Auxillary images generated for the Project in a separated Jupyter Notebook __[Auxillary Images](http://localhost:8888/notebooks/code/Auxillary%20Images.ipynb)__ are available as *.png files at the __[images](http://localhost:8888/tree/images)__ folder in the Projects repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dictionary ##\n",
    "\n",
    "Initial datasets used for the Project were obtained in the [Data Library of Bureau of Tranportation Statistics](https://www.transtats.bts.gov/Databases.asp?Mode_ID=1&Mode_Desc=Aviation&Subject_ID2=0) as a part of US Departament of Transportation and US Federal Aviation Authority Open Data Project. \n",
    "Flight performance information is available for download per monthly reporting periods with possibility to pre-select parameters of interest.\n",
    "A data dictionary for all parameters is available at [Data Dictionary](https://www.transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time) where all the fields and industry-specific terminology is explained in the sections corresponding to **\"Reporting Carrier On-Time Performance\"** section.\n",
    "Throuh the Project flow some additional explanations are given wherever necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary ##\n",
    "\n",
    "In order to analyse flight delays pattern I decided to pick the most recent period of Quarter 4, 2018. The last three months of each year are quite representative in terms of domestic travel - October is quite diverse weather-wise and includes school vacations nearly everywhere, November is even diverser weather-wise with increased winter storm disruptions and it also includes one of the highest travel period - Thanksgiving holidays. December is traditionally the most complicated month as weather always brings unpleasant surprises, it includes Christmas as another school vacations time everywhere in the US and another busiest time across the airports. \n",
    "\n",
    "Initial monthly domestic travel data file contain around 600_000 flights each month. The resulting databases are humongous and for the sake of simplification, speeding up of work flow and (!) ease of debugging the Project was divided into several separate Jupiter notebooks with inavoidably repetitive beginnings. \n",
    "\n",
    "Workflow explanations are given in accordance with the Project notebooks organization order. \n",
    "\n",
    "### 1. Importing Data ###\n",
    "\n",
    "Data was initially spread between the three monthly flight statistic files for October, November and december 2018 accordingly. In this Jupyter Notebook I did basical exploration of these csv files and corresponding Pandas DataFrames and eventually concatenated them into a joint csv file. \n",
    "\n",
    "### 2. Cleaning ###\n",
    "\n",
    "Data acquired from an external source naturally weren't ideally suited for planned research. In this notebook I did some standard data cleaning, as well as significantly reduced the amount of data features as most of them either didn't relate to the research or repeated other data set features. Also I had to change some data formats and address missing values resulted from data structure, as well as to take conceptual decisions to drop cancelled flights from the research entirely (they represent a different subset of flights and a topic for another possible research) and keep diverted flights in the datasets only if they eventually reached their final destination (not being cancelled at the diversion airport, as the ones that didn't reach their final destination and were cancelled at the diversion airport represent another distinct very small subset and dropping them in favor of simplifying research methodology made sence).\n",
    "Also, for the reasons of being more informative to the research' audience, I had to replace aircarrier codes with their respected full names.\n",
    "As well, I had to tackle some outliers - both positive and negative - so that the remaining dataset trends weren't influenced by them. \n",
    "\n",
    "### 3. EDA ###\n",
    "\n",
    "Explorative Data Analysis quite straight-forward.\n",
    "For the reasons of research target audience's convenience I had to re-attribute smaller feeder airlines flight to the larger carriers who actually act as a ticket seller and main point of customer's contact. Then I explored average delays by carrier and their reasons, as well as delays by reason breakdown.\n",
    "\n",
    "*It is important to note that the flights reported to FAA have several categories of delay, as well as total delay data. A sum of these delay categories is not always equal to the flight's total delay, and represents the **explained flight delay** as some other parts of the total delay might be either not reported into any of these categories, or there's no categories breakdown at all. This is a potential sourse of confusion.*\n",
    "\n",
    "Finally, in this section I had a look at our target variable distribution. \n",
    "\n",
    "### 4. Pre-Processing ###\n",
    "\n",
    "The purpose of this separate notebook is to create a dataset to be universally used for any modeling in future.\n",
    "\n",
    "In order to do so I looked at correlations between our remaining features, as well as dealt with imbalanced classes problem for classification models. To avoid inbalanced clasees with our future classification models I chose to construct my modeling dataset in a way that the positive and negative classes are balanced as 1:1. I randomly pulled observations out of our positive (flight on time) class - and a number of these observations will be equal to the number of tweets in our negative (flight delayed) class.\n",
    "\n",
    "### 5 Logistic Regression Classifier###\n",
    "\n",
    "Fitting a Logistic Regression Classifier on flights data is a classic approach to classifications problems. \n",
    "\n",
    "In my case it resulted in default model performing with accuracy of slightly above 0.57 which is just a light improvement for the Baseline model for balanced classed accuracy of 0.5.\n",
    "\n",
    "I attempted hyperparameters tuning and achieved just a very small, nearly negligible accuracy improvement.\n",
    "\n",
    "### 6 Decision Trees ###\n",
    "\n",
    "A default Decision Tree Classifier performed with accuracy of 1.0 on a training set, but just with 0.61 on a testing set. A Bagging Classifier showed similar results with training set accuracy of 0.98 and just 0.64 on a testing sets. Both models were showing signs of being severely overfit.\n",
    "\n",
    "A default Random Forest model showed signs of being very overfit performing with the accuracy of 0.99 on a train-set and 0.62 on a test-set. Hyperparameters grid-search allowed me to get rid of overfitting at a price of having my best grid-searched model accuracy of 0.59 on a training set and 0.58 on a testing set. This is not a significant improvement with both Logistic Regression and Baseline models.\n",
    "\n",
    "Fitting an AdaBoost Classifier with my best grid-searched Random Forest model as a base estimator permitted me to achieve accuracy of 0.68 on a training set and 0.63 on a testing set.\n",
    "\n",
    "Applying XGBoost technique resulted in accuracy of 0.64 on a training set and 0.63 on a testing set. \n",
    "\n",
    "Hence, AdaBoost Classifier using as a base estimator an optimized Random Forest model showed the highest accuracy scores both on training and testing data.\n",
    "\n",
    "Aslo, I was able to use my best performing model to determine the most important flight delay predictors. They turned out to be flight data (month, date and weekday, flight distance, flight number and operating carrier,as well as departure/arrival from certain airports. More information with some visuals is available in the Project presentation. An interesting fact is that the most powerful flight delay predictor among flight destinations is the airport of Newark, NJ and Chicago O'Hare airport among flight origins, respectedly.\n",
    "\n",
    "### 7 Forward Feeding Neural Network Classifier ###\n",
    "\n",
    "Fitting a FFNN with two hidden layers of 128 and 64 neurons resulted in max classification accuracy (after an early stop at 17 epoch) of 0.66 and validation accuracy of 0.62. This is outperformed by the AdaBoost Classifier.\n",
    "\n",
    "### 8 Forward Feeding Neural Network Regressor ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions ##\n",
    "\n",
    "## Future Steps ##\n",
    "\n",
    "## Source Documentation ##\n",
    "\n",
    "Images used in Project Presentation are either created within the Project or sourced at **[Wikimedia Commons](https://commons.wikimedia.org/wiki/Main_Page/)** and are copyright-free.\n",
    "\n",
    "Whenever appropriate, code credits are given in code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
